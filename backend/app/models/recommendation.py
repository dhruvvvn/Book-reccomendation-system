"""
Recommendation Pydantic Models

Schemas for the recommendation pipeline:
- Candidates: Pre-LLM search results with similarity scores
- Results: Post-LLM ranked recommendations with explanations
"""

from typing import Optional
from pydantic import BaseModel, Field

from app.models.book import BookInDB


class RecommendationCandidate(BaseModel):
    """
    A candidate book from vector search before LLM reranking.
    
    Contains the raw similarity score and metadata.
    These candidates are sent to the LLM for reranking.
    """
    book: BookInDB = Field(..., description="Book data")
    similarity_score: float = Field(
        ...,
        ge=0,
        le=1,
        description="Cosine similarity from vector search"
    )
    metadata_score: Optional[float] = Field(
        None,
        ge=0,
        le=1,
        description="Score from metadata filtering (rating, popularity)"
    )
    combined_score: Optional[float] = Field(
        None,
        ge=0,
        le=1,
        description="Weighted combination of similarity and metadata scores"
    )


class RecommendationResult(BaseModel):
    """
    Final recommendation after LLM reranking.
    
    This is what gets sent to the frontend. Includes the
    empathetic, context-aware explanation generated by the LLM.
    
    Design Decision:
    We keep book data flattened rather than nested to simplify
    frontend consumption and reduce response size.
    """
    # Book identification
    book_id: str = Field(..., description="Unique book identifier")
    title: str = Field(..., description="Book title")
    author: str = Field(..., description="Author name")
    
    # Book details
    description: str = Field(..., description="Book description")
    genre: str = Field(..., description="Primary genre")
    rating: float = Field(..., ge=0, le=5, description="Average rating")
    cover_url: Optional[str] = Field(None, description="Book cover image URL")
    
    # LLM-generated content
    explanation: str = Field(
        ...,
        description=(
            "Personalized explanation of why this book was recommended. "
            "Generated by the LLM based on user context and emotional state."
        )
    )
    relevance_reasons: Optional[list[str]] = Field(
        None,
        description="Bullet points explaining relevance to user's query"
    )
    
    # Ranking metadata
    rank: int = Field(..., ge=1, description="Position in recommendation list")
    confidence_score: Optional[float] = Field(
        None,
        ge=0,
        le=1,
        description="LLM's confidence in this recommendation"
    )
